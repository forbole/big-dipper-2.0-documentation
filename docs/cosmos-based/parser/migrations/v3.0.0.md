---
title: v2.0.0 to v3.0.0
sidebar_position: 2
---

## Migration to table partitioning
In order to efficiently query the database over the enormous amount of transactions and messages,
we've implemented PostgreSQL [Table Partitioning](https://www.postgresql.org/docs/10/ddl-partitioning.html) feature
starting from `v3.0.0` which turns the `transaction` and `message` tables into partitioned
tables and creates partitions by a certain interval of heights that can be configured in the 
[config.yaml file](./../config/config.md#database). 

### Update branch
Merge `v3.0.0` or `v3.0.0-stargate` tag to your BDjuno branch according to the chain's cosmos sdk base and fix all merge conflicts. 
Do __NOT__ install or restart it yet.
```
git fetch --tags
git checkout <your chain base>
git checkout -b merge/v3.0.0
```

Then, for cosmos sdk v0.44.x and later, run:
```
git merge v3.0.0
```
For stargate:
```
git merge v3.0.0-stargate
```

### Prepare for the migration
1. Add custom address parser in the file `bdjuno/database/migrate/utils.go` if any, 
or leave the slice empty if there's no custom address parse. 
```go
// If no custom address parser
var CustomAccountParser = []string{}
```
```go
// Take desmos chain as example:
var CustomAccountParser = []string{
	"sender", "receiver", "user", "counterparty", "blocker", "blocked",
}
```
2. Now, Run:
```shell
$ cd bdjuno
$ make install
$ bdjuno migrate v3
```
Two additional fields in `.bdjuno/config.yaml` will be created under the database section. 
Don't panic if you see the message pops up: `partition batch size is set to 0, skipping migration`, 
because these 2 fields need to be modified before starting the migration, for example:
```yaml
database:
    # How many blocks for each partition. 
    # In this case a partition is created per 100,000 blocks.
    partition_size: 100000
    
    # How many rows of transactions are migrated per batch 
    partition_batch: 1000 
```
Read [Database Config Reference](./../config/config.md#database) for more information.

### Start the migration
Run the below commands:
```shell
$ sudo systemctl stop bdjuno.service # Stop current bdjuno service
$ bdjuno migrate v3 # Start the migration
```
BDjuno will now start to migrate which will rename the tables `transaction` & `message` to `transaction_old` & `message_old`; 
rename the related indexes; create new tables `transaction` & `message` and new indexes with partitioning enabled; 
migrate rows from old to new tables by batch; and at last, drop the old function `messages_by_address` and create a new one.

While the migration is ongoing, you may already restart the bdjuno service which will insert into the new partitionen tables:
```shell
$ sudo systemctl restart bdjuno.service 
```


### Migration completed
If no error pops up, the migration is completed. For safety reason it does __not__ drop the old tables `transaction_old` and `message_old`. 
You might want to drop them after the data consistency is verified (or don't even drop them).

### Revert
In any case that the migration fails, you can always drop the new `transaction` & `message` tables and alter the old tables back.
You might find the below SQL script helpful for altering back the table names and index names:
```sql
ALTER TABLE transaction_old RENAME TO transaction;
ALTER INDEX transaction_old_pkey RENAME TO transaction_pkey;
ALTER INDEX transaction_old_hash_index RENAME TO transaction_hash_index;
ALTER INDEX transaction_old_height_index RENAME TO transaction_height_index;
ALTER TABLE transaction RENAME CONSTRAINT transaction_old_height_fkey TO transaction_height_fkey;

ALTER TABLE message_old RENAME TO message;
ALTER INDEX message_old_involved_accounts_addresses RENAME TO message_involved_accounts_addresses;
ALTER INDEX message_old_transaction_hash_index RENAME TO message_transaction_hash_index;
ALTER INDEX message_old_type_index RENAME TO message_type_index;
ALTER TABLE message RENAME CONSTRAINT message_old_transaction_hash_fkey TO message_transaction_hash_fkey;
```

When the tables are altered back, install and run your previous BDjuno branch which 
will continue to parse from the height before the migration started.